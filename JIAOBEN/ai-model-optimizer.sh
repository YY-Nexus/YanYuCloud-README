#!/bin/bash

# YYCÂ³ AI æ¨¡å‹ä¼˜åŒ–è„šæœ¬
# é…ç½® AI æ¨¡å‹è´Ÿè½½å‡è¡¡å’Œæ™ºèƒ½è·¯ç”±

set -e

ROOT_DIR="/volume2/YC"
AI_DIR="/volume2/YC/ai-models"
OLLAMA_DIR="/volume2/YC/ollama"
NAS_IP="192.168.3.45"

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[ä¿¡æ¯]${NC} $1"; }
log_success() { echo -e "${GREEN}[æˆåŠŸ]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[è­¦å‘Š]${NC} $1"; }
log_error() { echo -e "${RED}[é”™è¯¯]${NC} $1"; }
log_step() { echo -e "${PURPLE}[æ­¥éª¤]${NC} $1"; }
log_highlight() { echo -e "${CYAN}[é‡ç‚¹]${NC} $1"; }

# æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
show_welcome() {
    clear
    echo -e "${CYAN}"
    cat &lt;&lt; 'EOF'
    â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—
    â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ•â•â•â•â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘
     â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•  â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
      â•šâ–ˆâ–ˆâ•”â•    â•šâ–ˆâ–ˆâ•”â•  â–ˆâ–ˆâ•‘      â•šâ•â•â•â–ˆâ–ˆâ•—    â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
       â–ˆâ–ˆâ•‘      â–ˆâ–ˆâ•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•    â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘
       â•šâ•â•      â•šâ•â•    â•šâ•â•â•â•â•â•â•šâ•â•â•â•â•â•     â•šâ•â•  â•šâ•â•â•šâ•â•
                                                     
    YYCÂ³ AI æ¨¡å‹ä¼˜åŒ–
    AI Model Optimizer
    ==================
EOF
    echo -e "${NC}"
    echo ""
    echo "ğŸ¤– é…ç½® AI æ¨¡å‹è´Ÿè½½å‡è¡¡å’Œæ™ºèƒ½è·¯ç”±"
    echo "ğŸ“… ä¼˜åŒ–æ—¶é—´: $(date)"
    echo "ğŸŒ ç›®æ ‡æœåŠ¡å™¨: $NAS_IP"
    echo "ğŸ“ æ¨¡å‹ç›®å½•: $AI_DIR"
    echo ""
}

# åˆ›å»º AI æ¨¡å‹ç›®å½•ç»“æ„
create_ai_structure() {
    log_step "åˆ›å»º AI æ¨¡å‹ç›®å½•ç»“æ„..."
    
    mkdir -p "$AI_DIR"/{models,cache,logs,config}
    mkdir -p "$AI_DIR/models"/{ollama,openai,custom}
    mkdir -p "$AI_DIR/cache"/{embeddings,responses,sessions}
    mkdir -p "$OLLAMA_DIR"/{models,config}
    
    log_success "ç›®å½•ç»“æ„åˆ›å»ºå®Œæˆ"
}

# åˆ›å»º AI æ¨¡å‹è´Ÿè½½å‡è¡¡å™¨
create_load_balancer() {
    log_step "åˆ›å»º AI æ¨¡å‹è´Ÿè½½å‡è¡¡å™¨..."
    
    cat > "$AI_DIR/docker-compose.yml" &lt;&lt; 'EOF'
version: '3.8'

services:
  # Ollama æœåŠ¡é›†ç¾¤
  ollama-1:
    image: ollama/ollama:latest
    container_name: yc-ollama-1
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - /volume2/YC/ollama/models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    networks:
      - yyc3-network
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G

  ollama-2:
    image: ollama/ollama:latest
    container_name: yc-ollama-2
    restart: unless-stopped
    ports:
      - "11435:11434"
    volumes:
      - /volume2/YC/ollama/models:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    networks:
      - yyc3-network
    deploy:
      resources:
        limits:
          memory: 8G
        reservations:
          memory: 4G

  # AI è·¯ç”±å™¨
  ai-router:
    image: nginx:alpine
    container_name: yc-ai-router
    restart: unless-stopped
    ports:
      - "8888:80"
    volumes:
      - /volume2/YC/ai-models/config/nginx.conf:/etc/nginx/nginx.conf
      - /volume2/YC/ai-models/logs:/var/log/nginx
    networks:
      - yyc3-network
    depends_on:
      - ollama-1
      - ollama-2

  # AI ç›‘æ§
  ai-monitor:
    image: prom/prometheus:latest
    container_name: yc-ai-monitor
    restart: unless-stopped
    ports:
      - "9091:9090"
    volumes:
      - /volume2/YC/ai-models/config/prometheus.yml:/etc/prometheus/prometheus.yml
      - /volume2/YC/ai-models/config/rules:/etc/prometheus/rules
    networks:
      - yyc3-network
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'

  # Redis ç¼“å­˜
  ai-cache:
    image: redis:7-alpine
    container_name: yc-ai-cache
    restart: unless-stopped
    ports:
      - "6380:6379"
    volumes:
      - /volume2/YC/ai-models/cache:/data
    networks:
      - yyc3-network
    command: redis-server --appendonly yes --maxmemory 2gb --maxmemory-policy allkeys-lru

networks:
  yyc3-network:
    external: true
EOF

    log_success "è´Ÿè½½å‡è¡¡å™¨é…ç½®åˆ›å»ºå®Œæˆ"
}

# åˆ›å»º Nginx è·¯ç”±é…ç½®
create_nginx_config() {
    log_step "åˆ›å»º Nginx è·¯ç”±é…ç½®..."
    
    cat > "$AI_DIR/config/nginx.conf" &lt;&lt; 'EOF'
events {
    worker_connections 1024;
}

http {
    upstream ollama_backend {
        least_conn;
        server ollama-1:11434 weight=1 max_fails=3 fail_timeout=30s;
        server ollama-2:11434 weight=1 max_fails=3 fail_timeout=30s;
    }
    
    upstream openai_backend {
        server api.openai.com:443;
    }
    
    # æ—¥å¿—æ ¼å¼
    log_format ai_access '$remote_addr - $remote_user [$time_local] '
                        '"$request" $status $body_bytes_sent '
                        '"$http_referer" "$http_user_agent" '
                        'rt=$request_time uct="$upstream_connect_time" '
                        'uht="$upstream_header_time" urt="$upstream_response_time"';
    
    access_log /var/log/nginx/ai_access.log ai_access;
    error_log /var/log/nginx/ai_error.log;
    
    # é™æµé…ç½®
    limit_req_zone $binary_remote_addr zone=ai_limit:10m rate=10r/s;
    
    server {
        listen 80;
        server_name ai-router.yyc3.local;
        
        # å¥åº·æ£€æŸ¥
        location /health {
            access_log off;
            return 200 "healthy\n";
            add_header Content-Type text/plain;
        }
        
        # Ollama æœ¬åœ°æ¨¡å‹è·¯ç”±
        location /api/ollama/ {
            limit_req zone=ai_limit burst=20 nodelay;
            
            proxy_pass http://ollama_backend/;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
            
            # è¶…æ—¶è®¾ç½®
            proxy_connect_timeout 30s;
            proxy_send_timeout 300s;
            proxy_read_timeout 300s;
            
            # ç¼“å­˜è®¾ç½®
            proxy_cache_bypass $http_pragma;
            proxy_cache_revalidate on;
            proxy_cache_min_uses 1;
            proxy_cache_use_stale error timeout updating http_500 http_502 http_503 http_504;
        }
        
        # OpenAI API ä»£ç†
        location /api/openai/ {
            limit_req zone=ai_limit burst=10 nodelay;
            
            proxy_pass https://api.openai.com/;
            proxy_ssl_server_name on;
            proxy_set_header Host api.openai.com;
            proxy_set_header Authorization $http_authorization;
            
            # è¶…æ—¶è®¾ç½®
            proxy_connect_timeout 30s;
            proxy_send_timeout 60s;
            proxy_read_timeout 60s;
        }
        
        # æ™ºèƒ½è·¯ç”± - æ ¹æ®æ¨¡å‹ç±»å‹è‡ªåŠ¨é€‰æ‹©åç«¯
        location /api/chat {
            limit_req zone=ai_limit burst=15 nodelay;
            
            # ä½¿ç”¨ Lua è„šæœ¬è¿›è¡Œæ™ºèƒ½è·¯ç”±
            access_by_lua_block {
                local cjson = require "cjson"
                local http = require "resty.http"
                
                -- è¯»å–è¯·æ±‚ä½“
                ngx.req.read_body()
                local body = ngx.req.get_body_data()
                
                if body then
                    local ok, data = pcall(cjson.decode, body)
                    if ok and data.model then
                        -- æœ¬åœ°æ¨¡å‹è·¯ç”±åˆ° Ollama
                        if string.match(data.model, "llama") or 
                           string.match(data.model, "qwen") or
                           string.match(data.model, "mistral") then
                            ngx.var.backend = "ollama_backend"
                        else
                            -- å…¶ä»–æ¨¡å‹è·¯ç”±åˆ° OpenAI
                            ngx.var.backend = "openai_backend"
                        end
                    end
                end
            }
            
            proxy_pass http://$backend/v1/chat/completions;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }
        
        # æ¨¡å‹ç®¡ç†æ¥å£
        location /api/models {
            proxy_pass http://ollama_backend/api/tags;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
        }
        
        # ç»Ÿè®¡ä¿¡æ¯
        location /stats {
            stub_status on;
            access_log off;
            allow 192.168.0.0/24;
            deny all;
        }
    }
}
EOF

    log_success "Nginx é…ç½®åˆ›å»ºå®Œæˆ"
}

# åˆ›å»º Prometheus ç›‘æ§é…ç½®
create_prometheus_config() {
    log_step "åˆ›å»º Prometheus ç›‘æ§é…ç½®..."
    
    cat > "$AI_DIR/config/prometheus.yml" &lt;&lt; 'EOF'
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "rules/*.yml"

scrape_configs:
  - job_name: 'ai-router'
    static_configs:
      - targets: ['ai-router:80']
    metrics_path: /stats
    scrape_interval: 10s

  - job_name: 'ollama-1'
    static_configs:
      - targets: ['ollama-1:11434']
    metrics_path: /metrics
    scrape_interval: 30s

  - job_name: 'ollama-2'
    static_configs:
      - targets: ['ollama-2:11434']
    metrics_path: /metrics
    scrape_interval: 30s

  - job_name: 'redis'
    static_configs:
      - targets: ['ai-cache:6379']
    scrape_interval: 30s

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
EOF

    # åˆ›å»ºå‘Šè­¦è§„åˆ™
    mkdir -p "$AI_DIR/config/rules"
    
    cat > "$AI_DIR/config/rules/ai_alerts.yml" &lt;&lt; 'EOF'
groups:
  - name: ai_model_alerts
    rules:
      - alert: OllamaServiceDown
        expr: up{job="ollama-1"} == 0 or up{job="ollama-2"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Ollama æœåŠ¡ä¸å¯ç”¨"
          description: "Ollama æœåŠ¡ {{ $labels.instance }} å·²åœæ­¢å“åº”è¶…è¿‡ 1 åˆ†é’Ÿ"

      - alert: HighAIRequestLatency
        expr: nginx_http_request_duration_seconds{quantile="0.95"} > 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "AI è¯·æ±‚å»¶è¿Ÿè¿‡é«˜"
          description: "95% çš„ AI è¯·æ±‚å»¶è¿Ÿè¶…è¿‡ 10 ç§’"

      - alert: AIRequestRateHigh
        expr: rate(nginx_http_requests_total[5m]) > 100
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "AI è¯·æ±‚é¢‘ç‡è¿‡é«˜"
          description: "AI è¯·æ±‚é¢‘ç‡è¶…è¿‡æ¯ç§’ 100 æ¬¡"

      - alert: RedisMemoryUsageHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
          description: "Redis å†…å­˜ä½¿ç”¨ç‡è¶…è¿‡ 90%"
EOF

    log_success "Prometheus é…ç½®åˆ›å»ºå®Œæˆ"
}

# åˆ›å»º AI æ¨¡å‹ç®¡ç†è„šæœ¬
create_model_manager() {
    log_step "åˆ›å»º AI æ¨¡å‹ç®¡ç†è„šæœ¬..."
    
    cat > "$AI_DIR/manage-models.sh" &lt;&lt; 'EOF'
#!/bin/bash

# YYCÂ³ AI æ¨¡å‹ç®¡ç†è„šæœ¬

set -e

OLLAMA_HOST="http://192.168.3.45:11434"
AI_ROUTER="http://192.168.3.45:8888"

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[ä¿¡æ¯]${NC} $1"; }
log_success() { echo -e "${GREEN}[æˆåŠŸ]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[è­¦å‘Š]${NC} $1"; }
log_error() { echo -e "${RED}[é”™è¯¯]${NC} $1"; }

# æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
show_help() {
    echo "YYCÂ³ AI æ¨¡å‹ç®¡ç†å·¥å…·"
    echo ""
    echo "ç”¨æ³•: $0 [å‘½ä»¤] [å‚æ•°]"
    echo ""
    echo "å‘½ä»¤:"
    echo "  list                    åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹"
    echo "  pull <model>           ä¸‹è½½æ¨¡å‹"
    echo "  remove <model>         åˆ é™¤æ¨¡å‹"
    echo "  status                 æŸ¥çœ‹æœåŠ¡çŠ¶æ€"
    echo "  test <model>           æµ‹è¯•æ¨¡å‹"
    echo "  benchmark <model>      æ€§èƒ½æµ‹è¯•"
    echo "  install-recommended    å®‰è£…æ¨èæ¨¡å‹"
    echo ""
    echo "ç¤ºä¾‹:"
    echo "  $0 list"
    echo "  $0 pull llama3.2:3b"
    echo "  $0 test qwen2.5:7b"
    echo ""
}

# åˆ—å‡ºæ¨¡å‹
list_models() {
    log_info "è·å–æ¨¡å‹åˆ—è¡¨..."
    
    echo "æœ¬åœ°æ¨¡å‹:"
    curl -s "$OLLAMA_HOST/api/tags" | jq -r '.models[] | "\(.name) - \(.size/1024/1024/1024 | floor)GB"' 2>/dev/null || {
        log_error "æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡"
        return 1
    }
    
    echo ""
    echo "æ¨èæ¨¡å‹:"
    echo "  llama3.2:3b     - è½»é‡çº§å¯¹è¯æ¨¡å‹ (2GB)"
    echo "  qwen2.5:7b      - ä¸­æ–‡ä¼˜åŒ–æ¨¡å‹ (4GB)"
    echo "  mistral:7b      - é«˜æ€§èƒ½æ¨¡å‹ (4GB)"
    echo "  codellama:7b    - ä»£ç ç”Ÿæˆæ¨¡å‹ (4GB)"
}

# ä¸‹è½½æ¨¡å‹
pull_model() {
    local model="$1"
    if [ -z "$model" ]; then
        log_error "è¯·æŒ‡å®šæ¨¡å‹åç§°"
        return 1
    fi
    
    log_info "ä¸‹è½½æ¨¡å‹: $model"
    curl -X POST "$OLLAMA_HOST/api/pull" \
        -H "Content-Type: application/json" \
        -d "{\"name\":\"$model\"}" \
        --no-buffer | while IFS= read -r line; do
        echo "$line" | jq -r '.status // empty' 2>/dev/null || echo "$line"
    done
    
    log_success "æ¨¡å‹ä¸‹è½½å®Œæˆ: $model"
}

# åˆ é™¤æ¨¡å‹
remove_model() {
    local model="$1"
    if [ -z "$model" ]; then
        log_error "è¯·æŒ‡å®šæ¨¡å‹åç§°"
        return 1
    fi
    
    log_warning "åˆ é™¤æ¨¡å‹: $model"
    curl -X DELETE "$OLLAMA_HOST/api/delete" \
        -H "Content-Type: application/json" \
        -d "{\"name\":\"$model\"}"
    
    log_success "æ¨¡å‹åˆ é™¤å®Œæˆ: $model"
}

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
check_status() {
    log_info "æ£€æŸ¥æœåŠ¡çŠ¶æ€..."
    
    echo "Ollama æœåŠ¡:"
    for port in 11434 11435; do
        if curl -s "http://192.168.3.45:$port" > /dev/null; then
            echo "  âœ… Ollama ($port) - è¿è¡Œä¸­"
        else
            echo "  âŒ Ollama ($port) - åœæ­¢"
        fi
    done
    
    echo ""
    echo "AI è·¯ç”±å™¨:"
    if curl -s "$AI_ROUTER/health" > /dev/null; then
        echo "  âœ… AI Router - è¿è¡Œä¸­"
    else
        echo "  âŒ AI Router - åœæ­¢"
    fi
    
    echo ""
    echo "Redis ç¼“å­˜:"
    if redis-cli -h 192.168.3.45 -p 6380 ping > /dev/null 2>&1; then
        echo "  âœ… Redis - è¿è¡Œä¸­"
    else
        echo "  âŒ Redis - åœæ­¢"
    fi
}

# æµ‹è¯•æ¨¡å‹
test_model() {
    local model="$1"
    if [ -z "$model" ]; then
        log_error "è¯·æŒ‡å®šæ¨¡å‹åç§°"
        return 1
    fi
    
    log_info "æµ‹è¯•æ¨¡å‹: $model"
    
    local test_prompt="ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
    local start_time=$(date +%s.%N)
    
    local response=$(curl -s "$OLLAMA_HOST/api/generate" \
        -H "Content-Type: application/json" \
        -d "{\"model\":\"$model\",\"prompt\":\"$test_prompt\",\"stream\":false}")
    
    local end_time=$(date +%s.%N)
    local duration=$(echo "$end_time - $start_time" | bc)
    
    echo "å“åº”æ—¶é—´: ${duration}s"
    echo "æ¨¡å‹å“åº”:"
    echo "$response" | jq -r '.response' 2>/dev/null || echo "$response"
}

# æ€§èƒ½æµ‹è¯•
benchmark_model() {
    local model="$1"
    if [ -z "$model" ]; then
        log_error "è¯·æŒ‡å®šæ¨¡å‹åç§°"
        return 1
    fi
    
    log_info "æ€§èƒ½æµ‹è¯•: $model"
    
    local prompts=(
        "å†™ä¸€ä¸ª Hello World ç¨‹åº"
        "è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½"
        "è®¡ç®— 1+1 ç­‰äºå¤šå°‘"
        "ä»‹ç»ä¸€ä¸‹ YYCÂ³ å¼€å‘è€…å·¥å…·åŒ…"
        "ç”¨ JavaScript å†™ä¸€ä¸ªæ’åºå‡½æ•°"
    )
    
    local total_time=0
    local count=0
    
    for prompt in "${prompts[@]}"; do
        log_info "æµ‹è¯•æç¤º: $prompt"
        
        local start_time=$(date +%s.%N)
        curl -s "$OLLAMA_HOST/api/generate" \
            -H "Content-Type: application/json" \
            -d "{\"model\":\"$model\",\"prompt\":\"$prompt\",\"stream\":false}" > /dev/null
        local end_time=$(date +%s.%N)
        
        local duration=$(echo "$end_time - $start_time" | bc)
        total_time=$(echo "$total_time + $duration" | bc)
        count=$((count + 1))
        
        echo "  å“åº”æ—¶é—´: ${duration}s"
    done
    
    local avg_time=$(echo "scale=2; $total_time / $count" | bc)
    log_success "å¹³å‡å“åº”æ—¶é—´: ${avg_time}s"
}

# å®‰è£…æ¨èæ¨¡å‹
install_recommended() {
    log_info "å®‰è£…æ¨èæ¨¡å‹..."
    
    local models=(
        "llama3.2:3b"
        "qwen2.5:7b"
    )
    
    for model in "${models[@]}"; do
        log_info "å®‰è£…æ¨¡å‹: $model"
        pull_model "$model"
    done
    
    log_success "æ¨èæ¨¡å‹å®‰è£…å®Œæˆ"
}

# ä¸»å‡½æ•°
main() {
    case "${1:-help}" in
        "list")
            list_models
            ;;
        "pull")
            pull_model "$2"
            ;;
        "remove")
            remove_model "$2"
            ;;
        "status")
            check_status
            ;;
        "test")
            test_model "$2"
            ;;
        "benchmark")
            benchmark_model "$2"
            ;;
        "install-recommended")
            install_recommended
            ;;
        "help"|*)
            show_help
            ;;
    esac
}

main "$@"
EOF

    chmod +x "$AI_DIR/manage-models.sh"
    
    log_success "æ¨¡å‹ç®¡ç†è„šæœ¬åˆ›å»ºå®Œæˆ"
}

# å¯åŠ¨ AI æœåŠ¡
start_ai_services() {
    log_step "å¯åŠ¨ AI æœåŠ¡..."
    
    cd "$AI_DIR"
    
    # å¯åŠ¨æœåŠ¡
    docker-compose up -d
    
    # ç­‰å¾…æœåŠ¡å¯åŠ¨
    log_info "ç­‰å¾… AI æœåŠ¡å¯åŠ¨..."
    sleep 30
    
    # æ£€æŸ¥æœåŠ¡çŠ¶æ€
    if curl -s "http://$NAS_IP:8888/health" > /dev/null; then
        log_success "AI è·¯ç”±å™¨å¯åŠ¨æˆåŠŸ"
    else
        log_warning "AI è·¯ç”±å™¨å¯èƒ½æœªå®Œå…¨å¯åŠ¨"
    fi
    
    if curl -s "http://$NAS_IP:11434" > /dev/null; then
        log_success "Ollama æœåŠ¡å¯åŠ¨æˆåŠŸ"
    else
        log_warning "Ollama æœåŠ¡å¯èƒ½æœªå®Œå…¨å¯åŠ¨"
    fi
}

# ä¸»æ‰§è¡Œå‡½æ•°
main() {
    show_welcome
    
    # æ£€æŸ¥æƒé™
    if [[ $EUID -ne 0 ]]; then
        log_warning "å»ºè®®ä½¿ç”¨ root æƒé™è¿è¡Œæ­¤è„šæœ¬"
    fi
    
    # æ‰§è¡Œä¼˜åŒ–æ­¥éª¤
    create_ai_structure
    create_load_balancer
    create_nginx_config
    create_prometheus_config
    create_model_manager
    start_ai_services
    
    # æ˜¾ç¤ºå®Œæˆä¿¡æ¯
    echo ""
    log_success "ğŸ‰ YYCÂ³ AI æ¨¡å‹ä¼˜åŒ–å®Œæˆï¼"
    echo ""
    log_highlight "ğŸ“‹ æœåŠ¡æ‘˜è¦:"
    echo "  ğŸ¤– AI è·¯ç”±å™¨: http://$NAS_IP:8888"
    echo "  ğŸ¦™ Ollama 1: http://$NAS_IP:11434"
    echo "  ğŸ¦™ Ollama 2: http://$NAS_IP:11435"
    echo "  ğŸ“Š ç›‘æ§é¢æ¿: http://$NAS_IP:9091"
    echo "  ğŸ—„ï¸ Redis ç¼“å­˜: $NAS_IP:6380"
    echo ""
    log_highlight "ğŸš€ åç»­æ­¥éª¤:"
    echo "  1. è¿è¡Œ '$AI_DIR/manage-models.sh install-recommended' å®‰è£…æ¨èæ¨¡å‹"
    echo "  2. è¿è¡Œ '$AI_DIR/manage-models.sh status' æ£€æŸ¥æœåŠ¡çŠ¶æ€"
    echo "  3. è¿è¡Œ '$AI_DIR/manage-models.sh test llama3.2:3b' æµ‹è¯•æ¨¡å‹"
    echo ""
    log_highlight "ğŸ”§ ç®¡ç†å‘½ä»¤:"
    echo "  â€¢ æ¨¡å‹ç®¡ç†: $AI_DIR/manage-models.sh"
    echo "  â€¢ æœåŠ¡çŠ¶æ€: docker-compose -f $AI_DIR/docker-compose.yml ps"
    echo "  â€¢ æŸ¥çœ‹æ—¥å¿—: docker-compose -f $AI_DIR/docker-compose.yml logs -f"
    echo ""
}

# æ‰§è¡Œä¸»å‡½æ•°
main "$@"
