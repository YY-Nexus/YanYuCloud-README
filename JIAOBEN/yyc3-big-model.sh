#!/bin/bash

# YYCÂ³ AIæ¨¡åž‹é›†æˆç®¡ç†ç³»ç»Ÿ
# æ”¯æŒå›½å†…å¤–ä¸»æµå¤§æ¨¡åž‹ç»Ÿä¸€è°ƒç”¨å’Œç®¡ç†
# ä½œè€…ï¼šYYCÂ³
# ç‰ˆæœ¬ï¼š2.0
# æ›´æ–°æ—¥æœŸï¼š2025-07-12

# å¯ç”¨ä¸¥æ ¼æ¨¡å¼å’Œé”™è¯¯æ•èŽ·
set -euo pipefail
trap 'log_error "è„šæœ¬åœ¨ç¬¬ $LINENO è¡Œæ‰§è¡Œå¤±è´¥"; cleanup; exit 1' ERR

# åŸºç¡€é…ç½®ï¼ˆå¯é€šè¿‡äº¤äº’ä¿®æ”¹ï¼‰
ROOT_DIR="/volume1/YC"
NAS_IP="192.168.0.9"
LOG_DIR="$ROOT_DIR/logs"
AI_DIR="$ROOT_DIR/ai-models"
BACKUP_DIR="$ROOT_DIR/backups"
CONFIG_FILE="$AI_DIR/configs/system_config.json"

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
PURPLE='\033[0;35m'
CYAN='\033[0;36m'
NC='\033[0m'

# æ—¥å¿—å‡½æ•°
log_info() {
    local msg="[$(date '+%Y-%m-%d %H:%M:%S')] [INFO] $1"
    echo -e "${BLUE}${msg}${NC}"
    echo "${msg}" >> "$LOG_FILE"
}

log_success() {
    local msg="[$(date '+%Y-%m-%d %H:%M:%S')] [SUCCESS] $1"
    echo -e "${GREEN}${msg}${NC}"
    echo "${msg}" >> "$LOG_FILE"
}

log_warning() {
    local msg="[$(date '+%Y-%m-%d %H:%M:%S')] [WARNING] $1"
    echo -e "${YELLOW}${msg}${NC}"
    echo "${msg}" >> "$LOG_FILE"
}

log_error() {
    local msg="[$(date '+%Y-%m-%d %H:%M:%S')] [ERROR] $1"
    echo -e "${RED}${msg}${NC}"
    echo "${msg}" >> "$LOG_FILE"
}

log_step() {
    local msg="[$(date '+%Y-%m-%d %H:%M:%S')] [STEP] $1"
    echo -e "${PURPLE}${msg}${NC}"
    echo "${msg}" >> "$LOG_FILE"
}

# éªŒè¯IPåœ°å€
validate_ip() {
    local ip=$1
    if [[ ! "$ip" =~ ^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
        log_error "æ— æ•ˆçš„IPåœ°å€æ ¼å¼: $ip"
        return 1
    fi
    return 0
}

# éªŒè¯ç«¯å£
validate_port() {
    local port=$1
    if ! [[ "$port" =~ ^[0-9]+$ ]] || [ "$port" -lt 1 ] || [ "$port" -gt 65535 ]; then
        log_error "æ— æ•ˆçš„ç«¯å£: $port"
        return 1
    fi
    return 0
}

# æ£€æŸ¥ä¾èµ–
check_dependencies() {
    log_step "æ£€æŸ¥ç³»ç»Ÿä¾èµ–..."
    
    # æ£€æŸ¥åŸºç¡€å·¥å…·
    local basic_deps=("docker" "docker-compose" "curl" "wget" "git" "openssl" "python3" "pip3" "node" "npm")
    for dep in "${basic_deps[@]}"; do
        if ! command -v "$dep" &> /dev/null; then
            log_error "$dep æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£… $dep"
            
            # æä¾›å®‰è£…å»ºè®®
            if [ "$dep" = "docker" ]; then
                log_info "å®‰è£…å‘½ä»¤: curl -fsSL https://get.docker.com | sh"
            elif [ "$dep" = "node" ] || [ "$dep" = "npm" ]; then
                log_info "å®‰è£…å‘½ä»¤: curl -fsSL https://deb.nodesource.com/setup_18.x | sudo -E bash - && sudo apt-get install -y nodejs"
            elif [ "$dep" = "python3" ] || [ "$dep" = "pip3" ]; then
                log_info "å®‰è£…å‘½ä»¤: sudo apt-get install -y python3 python3-pip"
            fi
            
            exit 1
        fi
    done
    
    # æ£€æŸ¥Pythonåº“
    local python_deps=("torch" "transformers" "datasets" "accelerate" "wandb" "numpy" "pandas" "scikit-learn")
    for dep in "${python_deps[@]}"; do
        if ! pip3 list | grep -q "$dep"; then
            log_info "å®‰è£…Pythonåº“: $dep"
            if ! pip3 install "$dep"; then
                log_error "å®‰è£… $dep å¤±è´¥"
                exit 1
            fi
        fi
    done
    
    # æ£€æŸ¥Node.jsåº“
    local node_deps=("express" "axios" "cors" "jsonwebtoken" "dotenv" "winston")
    for dep in "${node_deps[@]}"; do
        if ! npm list -g | grep -q "$dep"; then
            log_info "å…¨å±€å®‰è£…Node.jsåº“: $dep"
            if ! npm install -g "$dep"; then
                log_error "å®‰è£… $dep å¤±è´¥"
                exit 1
            fi
        fi
    done
    
    log_success "ä¾èµ–æ£€æŸ¥é€šè¿‡"
}

# äº¤äº’å¼é…ç½®
interactive_config() {
    log_step "äº¤äº’å¼é…ç½®..."
    
    # é…ç½®æ ¹ç›®å½•
    read -p "è¯·è¾“å…¥æ ¹ç›®å½• (é»˜è®¤: $ROOT_DIR): " input_root
    if [ -n "$input_root" ]; then
        ROOT_DIR="$input_root"
    fi
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    mkdir -p "$ROOT_DIR"
    if [ ! -w "$ROOT_DIR" ]; then
        log_error "ç›®å½• $ROOT_DIR ä¸å¯å†™ï¼Œè¯·æ£€æŸ¥æƒé™"
        exit 1
    fi
    
    # é…ç½®NAS IP
    while true; do
        read -p "è¯·è¾“å…¥NAS IPåœ°å€ (é»˜è®¤: $NAS_IP): " input_ip
        local new_ip=${input_ip:-$NAS_IP}
        if validate_ip "$new_ip"; then
            NAS_IP="$new_ip"
            break
        fi
        log_warning "è¯·è¾“å…¥æœ‰æ•ˆçš„IPåœ°å€"
    done
    
    # é…ç½®ç«¯å£
    local gateway_port=3010
    while true; do
        read -p "è¯·è¾“å…¥AIç½‘å…³ç«¯å£ (é»˜è®¤: $gateway_port): " input_port
        local new_port=${input_port:-$gateway_port}
        if validate_port "$new_port"; then
            gateway_port="$new_port"
            break
        fi
        log_warning "è¯·è¾“å…¥æœ‰æ•ˆçš„ç«¯å£"
    done
    
    local dashboard_port=3011
    while true; do
        read -p "è¯·è¾“å…¥ç®¡ç†åŽå°ç«¯å£ (é»˜è®¤: $dashboard_port): " input_port
        local new_port=${input_port:-$dashboard_port}
        if validate_port "$new_port" && [ "$new_port" -ne "$gateway_port" ]; then
            dashboard_port="$new_port"
            break
        fi
        log_warning "è¯·è¾“å…¥æœ‰æ•ˆçš„ç«¯å£ï¼Œä¸”ä¸Žç½‘å…³ç«¯å£ä¸åŒ"
    done
    
    # ä¿å­˜é…ç½®
    save_config "ROOT_DIR" "$ROOT_DIR"
    save_config "NAS_IP" "$NAS_IP"
    save_config "GATEWAY_PORT" "$gateway_port"
    save_config "DASHBOARD_PORT" "$dashboard_port"
    save_config "LOG_DIR" "$LOG_DIR"
    save_config "BACKUP_DIR" "$BACKUP_DIR"
    
    log_success "é…ç½®å®Œæˆ"
}

# ä¿å­˜é…ç½®
save_config() {
    local key=$1
    local value=$2
    
    # ç¡®ä¿é…ç½®ç›®å½•å­˜åœ¨
    mkdir -p "$(dirname "$CONFIG_FILE")"
    
    # å¦‚æžœé…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ
    if [ ! -f "$CONFIG_FILE" ]; then
        echo "{}" > "$CONFIG_FILE"
    fi
    
    # ä½¿ç”¨jqæ›´æ–°é…ç½®
    if command -v jq &> /dev/null; then
        jq --arg key "$key" --arg value "$value" '.[$key] = $value' "$CONFIG_FILE" > "$CONFIG_FILE.tmp"
        mv "$CONFIG_FILE.tmp" "$CONFIG_FILE"
    else
        log_warning "jq æœªå®‰è£…ï¼Œæ— æ³•æ›´æ–°é…ç½®æ–‡ä»¶"
    fi
}

# åŠ è½½é…ç½®
load_config() {
    if [ -f "$CONFIG_FILE" ]; then
        log_step "åŠ è½½é…ç½®æ–‡ä»¶..."
        
        # è¯»å–é…ç½®å€¼
        if command -v jq &> /dev/null; then
            local root_dir=$(jq -r '.ROOT_DIR' "$CONFIG_FILE")
            local nas_ip=$(jq -r '.NAS_IP' "$CONFIG_FILE")
            local gateway_port=$(jq -r '.GATEWAY_PORT' "$CONFIG_FILE")
            local dashboard_port=$(jq -r '.DASHBOARD_PORT' "$CONFIG_FILE")
            
            # æ›´æ–°å˜é‡ï¼ˆå¦‚æžœé…ç½®æ–‡ä»¶ä¸­æœ‰å€¼ï¼‰
            if [ "$root_dir" != "null" ] && [ -n "$root_dir" ]; then
                ROOT_DIR="$root_dir"
            fi
            if [ "$nas_ip" != "null" ] && [ -n "$nas_ip" ]; then
                NAS_IP="$nas_ip"
            fi
            if [ "$gateway_port" != "null" ] && [ -n "$gateway_port" ]; then
                GATEWAY_PORT="$gateway_port"
            fi
            if [ "$dashboard_port" != "null" ] && [ -n "$dashboard_port" ]; then
                DASHBOARD_PORT="$dashboard_port"
            fi
        else
            log_warning "jq æœªå®‰è£…ï¼Œä½¿ç”¨é»˜è®¤é…ç½®"
        fi
    else
        log_warning "é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®"
    fi
    
    # æ›´æ–°ä¾èµ–è·¯å¾„
    AI_DIR="$ROOT_DIR/ai-models"
    LOG_DIR="$ROOT_DIR/logs"
    BACKUP_DIR="$ROOT_DIR/backups"
    CONFIG_FILE="$AI_DIR/configs/system_config.json"
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    mkdir -p "$AI_DIR" "$LOG_DIR" "$BACKUP_DIR"
}

# åˆ›å»ºç›®å½•ç»“æž„
create_directories() {
    log_step "åˆ›å»ºç›®å½•ç»“æž„..."
    
    # åˆ›å»ºä¸»ç›®å½•ç»“æž„
    mkdir -p "$AI_DIR"/{
        models/{local,remote,custom},
        configs,
        logs,
        cache,
        training/{datasets,models,checkpoints},
        deployment/{docker,kubernetes},
        api/{gateway,adapters},
        management/{web,api,nginx}
    }
    
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    mkdir -p "$LOG_DIR"/{gateway,trainer,dashboard,system}
    
    # åˆ›å»ºå¤‡ä»½ç›®å½•
    mkdir -p "$BACKUP_DIR"/{configs,models,data}
    
    log_success "ç›®å½•ç»“æž„åˆ›å»ºå®Œæˆ"
}

# åˆ›å»ºAIæ¨¡åž‹ç»Ÿä¸€ç½‘å…³æœåŠ¡
create_ai_gateway() {
    log_step "åˆ›å»ºAIæ¨¡åž‹ç»Ÿä¸€ç½‘å…³æœåŠ¡..."
    
    local gateway_dir="$AI_DIR/api/gateway"
    
    # åˆ›å»ºç½‘å…³ä»£ç 
    cat > "$gateway_dir/server.js" << 'EOF'
const express = require('express');
const cors = require('cors');
const axios = require('axios');
const fs = require('fs').promises;
const path = require('path');
const winston = require('winston');
const rateLimit = require('express-rate-limit');
const helmet = require('helmet');
require('dotenv').config();

// åˆå§‹åŒ–æ—¥å¿—
const logger = winston.createLogger({
    level: 'info',
    format: winston.format.combine(
        winston.format.timestamp(),
        winston.format.json()
    ),
    transports: [
        new winston.transports.File({ filename: path.join(__dirname, '../../logs/gateway/error.log'), level: 'error' }),
        new winston.transports.File({ filename: path.join(__dirname, '../../logs/gateway/combined.log') })
    ]
});

if (process.env.NODE_ENV !== 'production') {
    logger.add(new winston.transports.Console({
        format: winston.format.simple()
    }));
}

const app = express();
const PORT = process.env.AI_GATEWAY_PORT || 3010;
const NAS_IP = process.env.NAS_IP || '192.168.0.9';

// å®‰å…¨ä¸­é—´ä»¶
app.use(helmet());
app.use(cors());
app.use(express.json({ limit: '50mb' }));
app.use(express.urlencoded({ extended: true, limit: '50mb' }));

// é€ŸçŽ‡é™åˆ¶
const apiLimiter = rateLimit({
    windowMs: 15 * 60 * 1000, // 15åˆ†é’Ÿ
    max: 100, // æ¯ä¸ªIPé™åˆ¶100è¯·æ±‚
    standardHeaders: true,
    legacyHeaders: false,
    message: { error: 'è¯·æ±‚è¿‡äºŽé¢‘ç¹ï¼Œè¯·ç¨åŽå†è¯•' }
});
app.use('/api/', apiLimiter);

// æ¨¡åž‹é…ç½®å­˜å‚¨
let modelConfigs = {};
let modelStats = {};
let configPath = path.join(__dirname, '../../configs/models.json');

// åŠ è½½æ¨¡åž‹é…ç½®
async function loadModelConfigs() {
    try {
        const data = await fs.readFile(configPath, 'utf8');
        modelConfigs = JSON.parse(data);
        logger.info('âœ… æ¨¡åž‹é…ç½®åŠ è½½æˆåŠŸ');
    } catch (error) {
        logger.warn('âš ï¸ æ¨¡åž‹é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®');
        modelConfigs = getDefaultModelConfigs();
        await saveModelConfigs();
    }
}

// ä¿å­˜æ¨¡åž‹é…ç½®
async function saveModelConfigs() {
    try {
        await fs.writeFile(configPath, JSON.stringify(modelConfigs, null, 2));
        logger.info('âœ… æ¨¡åž‹é…ç½®ä¿å­˜æˆåŠŸ');
    } catch (error) {
        logger.error('âŒ ä¿å­˜æ¨¡åž‹é…ç½®å¤±è´¥:', error);
    }
}

// é»˜è®¤æ¨¡åž‹é…ç½®
function getDefaultModelConfigs() {
    return {
        "zhipu-ai": {
            name: "æ™ºè°±AI",
            type: "remote",
            endpoint: "https://open.bigmodel.cn/api/paas/v4/chat/completions",
            apiKey: process.env.ZHIPU_API_KEY || "",
            models: ["glm-4", "glm-4v", "glm-3-turbo"],
            enabled: true,
            rateLimit: 100,
            timeout: 30000
        },
        "openai": {
            name: "OpenAI",
            type: "remote",
            endpoint: "https://api.openai.com/v1/chat/completions",
            apiKey: process.env.OPENAI_API_KEY || "",
            models: ["gpt-4", "gpt-3.5-turbo", "gpt-4-turbo"],
            enabled: true,
            rateLimit: 60,
            timeout: 30000
        },
        "claude": {
            name: "Claude",
            type: "remote",
            endpoint: "https://api.anthropic.com/v1/messages",
            apiKey: process.env.CLAUDE_API_KEY || "",
            models: ["claude-3-opus", "claude-3-sonnet", "claude-3-haiku"],
            enabled: true,
            rateLimit: 50,
            timeout: 30000
        },
        "qwen": {
            name: "é€šä¹‰åƒé—®",
            type: "remote",
            endpoint: "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation",
            apiKey: process.env.QWEN_API_KEY || "",
            models: ["qwen-turbo", "qwen-plus", "qwen-max"],
            enabled: true,
            rateLimit: 100,
            timeout: 30000
        },
        "baichuan": {
            name: "ç™¾å·æ™ºèƒ½",
            type: "remote",
            endpoint: "https://api.baichuan-ai.com/v1/chat/completions",
            apiKey: process.env.BAICHUAN_API_KEY || "",
            models: ["Baichuan2-Turbo", "Baichuan2-Turbo-192k"],
            enabled: true,
            rateLimit: 100,
            timeout: 30000
        },
        "moonshot": {
            name: "æœˆä¹‹æš—é¢",
            type: "remote",
            endpoint: "https://api.moonshot.cn/v1/chat/completions",
            apiKey: process.env.MOONSHOT_API_KEY || "",
            models: ["moonshot-v1-8k", "moonshot-v1-32k", "moonshot-v1-128k"],
            enabled: true,
            rateLimit: 100,
            timeout: 30000
        },
        "ollama": {
            name: "æœ¬åœ°Ollama",
            type: "local",
            endpoint: "http://localhost:11434/api/chat",
            apiKey: "",
            models: ["llama2", "codellama", "mistral", "qwen"],
            enabled: true,
            rateLimit: 1000,
            timeout: 60000
        }
    };
}

// ç»Ÿä¸€èŠå¤©æŽ¥å£
app.post('/api/chat', async (req, res) => {
    try {
        const { model, messages, provider, stream = false, ...options } = req.body;
        
        if (!model || !messages) {
            return res.status(400).json({
                error: 'ç¼ºå°‘å¿…è¦å‚æ•°: model å’Œ messages'
            });
        }

        // æŸ¥æ‰¾æ¨¡åž‹é…ç½®
        const modelConfig = findModelConfig(model, provider);
        if (!modelConfig) {
            return res.status(404).json({
                error: `æœªæ‰¾åˆ°æ¨¡åž‹é…ç½®: ${model}`
            });
        }

        // æ£€æŸ¥æ¨¡åž‹æ˜¯å¦å¯ç”¨
        if (!modelConfig.enabled) {
            return res.status(403).json({
                error: `æ¨¡åž‹å·²ç¦ç”¨: ${model}`
            });
        }

        // è®°å½•è¯·æ±‚ç»Ÿè®¡
        recordModelUsage(modelConfig.name, model);

        // è®°å½•è¯·æ±‚æ—¥å¿—
        logger.info(`æ”¶åˆ°è¯·æ±‚: ${modelConfig.name} - ${model}`);

        // æ ¹æ®æä¾›å•†è°ƒç”¨ç›¸åº”çš„é€‚é…å™¨
        const response = await callModelAdapter(modelConfig, {
            model,
            messages,
            stream,
            ...options
        });

        res.json(response);

    } catch (error) {
        logger.error('âŒ èŠå¤©è¯·æ±‚å¤±è´¥:', error);
        res.status(500).json({
            error: 'å†…éƒ¨æœåŠ¡å™¨é”™è¯¯',
            details: process.env.NODE_ENV === 'development' ? error.message : undefined
        });
    }
});

// æŸ¥æ‰¾æ¨¡åž‹é…ç½®
function findModelConfig(model, provider) {
    if (provider && modelConfigs[provider]) {
        return modelConfigs[provider];
    }
    
    // éåŽ†æ‰€æœ‰é…ç½®æŸ¥æ‰¾æ¨¡åž‹
    for (const [key, config] of Object.entries(modelConfigs)) {
        if (config.models.includes(model)) {
            return config;
        }
    }
    
    return null;
}

// è°ƒç”¨æ¨¡åž‹é€‚é…å™¨
async function callModelAdapter(config, params) {
    switch (config.type) {
        case 'remote':
            return await callRemoteModel(config, params);
        case 'local':
            return await callLocalModel(config, params);
        default:
            throw new Error(`ä¸æ”¯æŒçš„æ¨¡åž‹ç±»åž‹: ${config.type}`);
    }
}

// è°ƒç”¨è¿œç¨‹æ¨¡åž‹
async function callRemoteModel(config, params) {
    const headers = {
        'Content-Type': 'application/json'
    };

    // æ ¹æ®ä¸åŒæä¾›å•†è®¾ç½®è®¤è¯å¤´
    if (config.name === 'æ™ºè°±AI') {
        headers['Authorization'] = `Bearer ${config.apiKey}`;
    } else if (config.name === 'OpenAI') {
        headers['Authorization'] = `Bearer ${config.apiKey}`;
    } else if (config.name === 'Claude') {
        headers['x-api-key'] = config.apiKey;
        headers['anthropic-version'] = '2023-06-01';
    } else if (config.name === 'é€šä¹‰åƒé—®') {
        headers['Authorization'] = `Bearer ${config.apiKey}`;
    } else {
        headers['Authorization'] = `Bearer ${config.apiKey}`;
    }

    // è½¬æ¢è¯·æ±‚æ ¼å¼
    const requestBody = transformRequest(config.name, params);

    try {
        const response = await axios.post(config.endpoint, requestBody, {
            headers,
            timeout: config.timeout
        });

        // è½¬æ¢å“åº”æ ¼å¼
        return transformResponse(config.name, response.data);

    } catch (error) {
        logger.error(`âŒ è°ƒç”¨${config.name}å¤±è´¥:`, error.response?.data || error.message);
        throw new Error(`è°ƒç”¨${config.name}å¤±è´¥: ${error.response?.data?.error?.message || error.message}`);
    }
}

// è°ƒç”¨æœ¬åœ°æ¨¡åž‹
async function callLocalModel(config, params) {
    try {
        const response = await axios.post(config.endpoint, {
            model: params.model,
            messages: params.messages,
            stream: false
        }, {
            timeout: config.timeout
        });

        return {
            id: `local-${Date.now()}`,
            object: 'chat.completion',
            created: Math.floor(Date.now() / 1000),
            model: params.model,
            choices: [{
                index: 0,
                message: {
                    role: 'assistant',
                    content: response.data.message?.content || response.data.response
                },
                finish_reason: 'stop'
            }],
            usage: {
                prompt_tokens: 0,
                completion_tokens: 0,
                total_tokens: 0
            }
        };

    } catch (error) {
        logger.error('âŒ è°ƒç”¨æœ¬åœ°æ¨¡åž‹å¤±è´¥:', error);
        throw new Error(`è°ƒç”¨æœ¬åœ°æ¨¡åž‹å¤±è´¥: ${error.message}`);
    }
}

// è¯·æ±‚æ ¼å¼è½¬æ¢
function transformRequest(provider, params) {
    switch (provider) {
        case 'æ™ºè°±AI':
            return {
                model: params.model,
                messages: params.messages,
                temperature: params.temperature || 0.7,
                max_tokens: params.max_tokens || 1000,
                stream: params.stream || false
            };
        
        case 'Claude':
            return {
                model: params.model,
                max_tokens: params.max_tokens || 1000,
                messages: params.messages,
                temperature: params.temperature || 0.7
            };
        
        case 'é€šä¹‰åƒé—®':
            return {
                model: params.model,
                input: {
                    messages: params.messages
                },
                parameters: {
                    temperature: params.temperature || 0.7,
                    max_tokens: params.max_tokens || 1000
                }
            };
        
        default:
            return {
                model: params.model,
                messages: params.messages,
                temperature: params.temperature || 0.7,
                max_tokens: params.max_tokens || 1000,
                stream: params.stream || false
            };
    }
}

// å“åº”æ ¼å¼è½¬æ¢
function transformResponse(provider, data) {
    switch (provider) {
        case 'é€šä¹‰åƒé—®':
            return {
                id: data.request_id,
                object: 'chat.completion',
                created: Math.floor(Date.now() / 1000),
                model: data.output?.model || 'qwen',
                choices: [{
                    index: 0,
                    message: {
                        role: 'assistant',
                        content: data.output?.text || ''
                    },
                    finish_reason: data.output?.finish_reason || 'stop'
                }],
                usage: data.usage || {}
            };
        
        default:
            return data;
    }
}

// è®°å½•æ¨¡åž‹ä½¿ç”¨ç»Ÿè®¡
function recordModelUsage(provider, model) {
    const key = `${provider}-${model}`;
    if (!modelStats[key]) {
        modelStats[key] = {
            provider,
            model,
            requests: 0,
            lastUsed: null
        };
    }
    
    modelStats[key].requests++;
    modelStats[key].lastUsed = new Date().toISOString();
}

// èŽ·å–æ¨¡åž‹åˆ—è¡¨
app.get('/api/models', (req, res) => {
    const models = [];
    
    for (const [key, config] of Object.entries(modelConfigs)) {
        if (config.enabled) {
            config.models.forEach(model => {
                models.push({
                    id: model,
                    name: model,
                    provider: config.name,
                    type: config.type,
                    description: `${config.name} - ${model}`
                });
            });
        }
    }
    
    res.json({ models });
});

// èŽ·å–æ¨¡åž‹ç»Ÿè®¡
app.get('/api/stats', (req, res) => {
    res.json({
        providers: Object.keys(modelConfigs).length,
        models: Object.values(modelConfigs).reduce((sum, config) => sum + config.models.length, 0),
        usage: modelStats,
        uptime: process.uptime()
    });
});

// ç®¡ç†æŽ¥å£ - èŽ·å–é…ç½®
app.get('/api/admin/configs', (req, res) => {
    res.json(modelConfigs);
});

// ç®¡ç†æŽ¥å£ - æ›´æ–°é…ç½®
app.put('/api/admin/configs', async (req, res) => {
    try {
        modelConfigs = req.body;
        await saveModelConfigs();
        res.json({ success: true, message: 'é…ç½®æ›´æ–°æˆåŠŸ' });
    } catch (error) {
        logger.error('æ›´æ–°é…ç½®å¤±è´¥:', error);
        res.status(500).json({ error: 'é…ç½®æ›´æ–°å¤±è´¥', details: process.env.NODE_ENV === 'development' ? error.message : undefined });
    }
});

// å¥åº·æ£€æŸ¥
app.get('/health', (req, res) => {
    res.json({
        status: 'healthy',
        timestamp: new Date().toISOString(),
        uptime: process.uptime(),
        models: Object.keys(modelConfigs).length
    });
});

// å¯åŠ¨æœåŠ¡å™¨
async function startServer() {
    await loadModelConfigs();
    
    app.listen(PORT, '0.0.0.0', () => {
        logger.info(`ðŸ¤– AIæ¨¡åž‹ç½‘å…³æœåŠ¡å¯åŠ¨æˆåŠŸ`);
        logger.info(`ðŸŒ æœåŠ¡åœ°å€: http://0.0.0.0:${PORT}`);
        logger.info(`ðŸ“Š ç®¡ç†é¢æ¿: http://0.0.0.0:${PORT}/admin`);
        logger.info(`ðŸ” å¥åº·æ£€æŸ¥: http://0.0.0.0:${PORT}/health`);
    });
}

// å¯åŠ¨æœåŠ¡
startServer().catch(error => {
    logger.error('å¯åŠ¨æœåŠ¡å™¨å¤±è´¥:', error);
    process.exit(1);
});
EOF

    # åˆ›å»ºç½‘å…³Dockerfile
    cat > "$gateway_dir/Dockerfile" << 'EOF'
FROM node:18-alpine

WORKDIR /app

COPY package*.json ./
RUN npm install --production

COPY . .

ENV NODE_ENV=production
ENV AI_GATEWAY_PORT=3010

EXPOSE 3010

CMD ["node", "server.js"]
EOF

    # åˆ›å»ºç½‘å…³package.json
    cat > "$gateway_dir/package.json" << 'EOF'
{
  "name": "yyc3-ai-gateway",
  "version": "1.0.0",
  "description": "YYCÂ³ AIæ¨¡åž‹ç»Ÿä¸€ç½‘å…³",
  "main": "server.js",
  "scripts": {
    "start": "node server.js",
    "dev": "nodemon server.js",
    "test": "jest"
  },
  "dependencies": {
    "express": "^4.18.2",
    "cors": "^2.8.5",
    "axios": "^1.6.2",
    "winston": "^3.11.0",
    "express-rate-limit": "^7.1.4",
    "helmet": "^7.1.0",
    "dotenv": "^16.3.1"
  },
  "devDependencies": {
    "nodemon": "^3.0.2",
    "jest": "^29.7.0",
    "supertest": "^6.3.3"
  }
}
EOF

    log_success "AIæ¨¡åž‹ç»Ÿä¸€ç½‘å…³æœåŠ¡åˆ›å»ºå®Œæˆ"
}

# åˆ›å»ºæ™ºè°±AIä¸“é¡¹é›†æˆ
create_zhipu_integration() {
    log_step "åˆ›å»ºæ™ºè°±AIä¸“é¡¹é›†æˆ..."
    
    local zhipu_dir="$AI_DIR/api/adapters"
    mkdir -p "$zhipu_dir"
    
    cat > "$zhipu_dir/zhipu-ai.js" << 'EOF'
const axios = require('axios');
const crypto = require('crypto');
const winston = require('winston');
const path = require('path');

// é…ç½®æ—¥å¿—
const logger = winston.createLogger({
    level: 'info',
    format: winston.format.combine(
        winston.format.timestamp(),
        winston.format.json()
    ),
    transports: [
        new winston.transports.File({ filename: path.join(__dirname, '../../logs/gateway/zhipu-error.log'), level: 'error' }),
        new winston.transports.File({ filename: path.join(__dirname, '../../logs/gateway/zhipu-combined.log') })
    ]
});

if (process.env.NODE_ENV !== 'production') {
    logger.add(new winston.transports.Console({
        format: winston.format.simple()
    }));
}

class ZhipuAI {
    constructor(apiKey) {
        this.apiKey = apiKey;
        this.baseURL = 'https://open.bigmodel.cn/api/paas/v4';
        this.stats = {
            totalRequests: 0,
            successfulRequests: 0,
            failedRequests: 0,
            totalTokens: 0,
            avgResponseTime: 0
        };
    }

    // ç”ŸæˆJWT Token
    generateToken() {
        const [apiKey, secret] = this.apiKey.split('.');
        
        const header = {
            alg: 'HS256',
            sign_type: 'SIGN'
        };
        
        const payload = {
            api_key: apiKey,
            exp: Math.floor(Date.now() / 1000) + 3600, // 1å°æ—¶è¿‡æœŸ
            timestamp: Math.floor(Date.now() / 1000)
        };
        
        const headerBase64 = Buffer.from(JSON.stringify(header)).toString('base64url');
        const payloadBase64 = Buffer.from(JSON.stringify(payload)).toString('base64url');
        
        const signature = crypto
            .createHmac('sha256', secret)
            .update(`${headerBase64}.${payloadBase64}`)
            .digest('base64url');
        
        return `${headerBase64}.${payloadBase64}.${signature}`;
    }

    // èŠå¤©å®Œæˆ
    async chatCompletions(params) {
        const startTime = Date.now();
        this.stats.totalRequests++;
        
        try {
            const token = this.generateToken();
            
            const response = await axios.post(`${this.baseURL}/chat/completions`, {
                model: params.model || 'glm-4',
                messages: params.messages,
                temperature: params.temperature || 0.7,
                max_tokens: params.max_tokens || 1000,
                stream: params.stream || false,
                tools: params.tools,
                tool_choice: params.tool_choice
            }, {
                headers: {
                    'Authorization': `Bearer ${token}`,
                    'Content-Type': 'application/json'
                },
                timeout: 30000
            });

            // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            this.stats.successfulRequests++;
            const responseTime = Date.now() - startTime;
            this.stats.avgResponseTime = (this.stats.avgResponseTime * (this.stats.successfulRequests - 1) + responseTime) / this.stats.successfulRequests;
            
            // è®°å½•tokenä½¿ç”¨
            if (response.data.usage) {
                this.stats.totalTokens += response.data.usage.total_tokens;
            }

            logger.info(`æ™ºè°±AIè¯·æ±‚æˆåŠŸ: ${params.model}, è€—æ—¶: ${responseTime}ms`);
            return response.data;

        } catch (error) {
            this.stats.failedRequests++;
            logger.error('æ™ºè°±AIè°ƒç”¨å¤±è´¥:', error.response?.data || error.message);
            throw new Error(`æ™ºè°±AIè°ƒç”¨å¤±è´¥: ${error.response?.data?.error?.message || error.message}`);
        }
    }

    // å›¾åƒç†è§£ (GLM-4V)
    async imageUnderstanding(params) {
        const startTime = Date.now();
        this.stats.totalRequests++;
        
        try {
            const token = this.generateToken();
            
            const messages = params.messages.map(msg => {
                if (msg.role === 'user' && Array.isArray(msg.content)) {
                    return {
                        role: 'user',
                        content: msg.content.map(item => {
                            if (item.type === 'image_url') {
                                return {
                                    type: 'image_url',
                                    image_url: {
                                        url: item.image_url.url
                                    }
                                };
                            }
                            return item;
                        })
                    };
                }
                return msg;
            });

            const response = await axios.post(`${this.baseURL}/chat/completions`, {
                model: 'glm-4v',
                messages: messages,
                temperature: params.temperature || 0.7,
                max_tokens: params.max_tokens || 1000
            }, {
                headers: {
                    'Authorization': `Bearer ${token}`,
                    'Content-Type': 'application/json'
                },
                timeout: 30000
            });

            // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            this.stats.successfulRequests++;
            const responseTime = Date.now() - startTime;
            this.stats.avgResponseTime = (this.stats.avgResponseTime * (this.stats.successfulRequests - 1) + responseTime) / this.stats.successfulRequests;
            
            logger.info(`æ™ºè°±AIå›¾åƒç†è§£æˆåŠŸ, è€—æ—¶: ${responseTime}ms`);
            return response.data;

        } catch (error) {
            this.stats.failedRequests++;
            logger.error('æ™ºè°±AIå›¾åƒç†è§£å¤±è´¥:', error.response?.data || error.message);
            throw new Error(`æ™ºè°±AIå›¾åƒç†è§£å¤±è´¥: ${error.response?.data?.error?.message || error.message}`);
        }
    }

    // ä»£ç ç”Ÿæˆ
    async codeGeneration(params) {
        const startTime = Date.now();
        this.stats.totalRequests++;
        
        try {
            const token = this.generateToken();
            
            const response = await axios.post(`${this.baseURL}/chat/completions`, {
                model: params.model || 'codegeex-4',
                messages: [
                    {
                        role: 'system',
                        content: 'ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä»£ç ç”ŸæˆåŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·éœ€æ±‚ç”Ÿæˆé«˜è´¨é‡çš„ä»£ç ã€‚'
                    },
                    ...params.messages
                ],
                temperature: params.temperature || 0.1,
                max_tokens: params.max_tokens || 2000
            }, {
                headers: {
                    'Authorization': `Bearer ${token}`,
                    'Content-Type': 'application/json'
                },
                timeout: 30000
            });

            // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            this.stats.successfulRequests++;
            const responseTime = Date.now() - startTime;
            this.stats.avgResponseTime = (this.stats.avgResponseTime * (this.stats.successfulRequests - 1) + responseTime) / this.stats.successfulRequests;
            
            logger.info(`æ™ºè°±AIä»£ç ç”ŸæˆæˆåŠŸ: ${params.model}, è€—æ—¶: ${responseTime}ms`);
            return response.data;

        } catch (error) {
            this.stats.failedRequests++;
            logger.error('æ™ºè°±AIä»£ç ç”Ÿæˆå¤±è´¥:', error.response?.data || error.message);
            throw new Error(`æ™ºè°±AIä»£ç ç”Ÿæˆå¤±è´¥: ${error.response?.data?.error?.message || error.message}`);
        }
    }

    // æ–‡æ¡£é—®ç­”
    async documentQA(params) {
        const startTime = Date.now();
        this.stats.totalRequests++;
        
        try {
            const token = this.generateToken();
            
            // æž„å»ºåŒ…å«æ–‡æ¡£ä¸Šä¸‹æ–‡çš„æ¶ˆæ¯
            const contextMessage = {
                role: 'system',
                content: `åŸºäºŽä»¥ä¸‹æ–‡æ¡£å†…å®¹å›žç­”ç”¨æˆ·é—®é¢˜ï¼š\n\n${params.document}\n\nè¯·ç¡®ä¿ç­”æ¡ˆå‡†ç¡®ä¸”åŸºäºŽæ–‡æ¡£å†…å®¹ã€‚`
            };

            const response = await axios.post(`${this.baseURL}/chat/completions`, {
                model: params.model || 'glm-4',
                messages: [contextMessage, ...params.messages],
                temperature: params.temperature || 0.3,
                max_tokens: params.max_tokens || 1500
            }, {
                headers: {
                    'Authorization': `Bearer ${token}`,
                    'Content-Type': 'application/json'
                },
                timeout: 30000
            });

            // æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            this.stats.successfulRequests++;
            const responseTime = Date.now() - startTime;
            this.stats.avgResponseTime = (this.stats.avgResponseTime * (this.stats.successfulRequests - 1) + responseTime) / this.stats.successfulRequests;
            
            logger.info(`æ™ºè°±AIæ–‡æ¡£é—®ç­”æˆåŠŸ: ${params.model}, è€—æ—¶: ${responseTime}ms`);
            return response.data;

        } catch (error) {
            this.stats.failedRequests++;
            logger.error('æ™ºè°±AIæ–‡æ¡£é—®ç­”å¤±è´¥:', error.response?.data || error.message);
            throw new Error(`æ™ºè°±AIæ–‡æ¡£é—®ç­”å¤±è´¥: ${error.response?.data?.error?.message || error.message}`);
        }
    }

    // èŽ·å–æ¨¡åž‹åˆ—è¡¨
    async getModels() {
        try {
            const token = this.generateToken();
            
            const response = await axios.get(`${this.baseURL}/models`, {
                headers: {
                    'Authorization': `Bearer ${token}`
                }
            });

            return response.data;

        } catch (error) {
            logger.error('èŽ·å–æ™ºè°±AIæ¨¡åž‹åˆ—è¡¨å¤±è´¥:', error.response?.data || error.message);
            throw new Error(`èŽ·å–æ™ºè°±AIæ¨¡åž‹åˆ—è¡¨å¤±è´¥: ${error.response?.data?.error?.message || error.message}`);
        }
    }

    // æ£€æŸ¥APIçŠ¶æ€
    async checkStatus() {
        try {
            const models = await this.getModels();
            return {
                status: 'healthy',
                models: models.data?.length || 0,
                timestamp: new Date().toISOString(),
                stats: this.stats
            };
        } catch (error) {
            return {
                status: 'error',
                error: error.message,
                timestamp: new Date().toISOString(),
                stats: this.stats
            };
        }
    }
}

module.exports = ZhipuAI;
EOF

    log_success "æ™ºè°±AIä¸“é¡¹é›†æˆåˆ›å»ºå®Œæˆ"
}

# åˆ›å»ºè‡ªå®šä¹‰æ¨¡åž‹è®­ç»ƒç³»ç»Ÿ
create_custom_training() {
    log_step "åˆ›å»ºè‡ªå®šä¹‰æ¨¡åž‹è®­ç»ƒç³»ç»Ÿ..."
    
    local training_dir="$AI_DIR/training"
    
    cat > "$training_dir/trainer.py" << 'EOF'
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import torch
import logging
import argparse
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass
from transformers import (
    AutoTokenizer, 
    AutoModelForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    BitsAndBytesConfig
)
from datasets import Dataset, load_dataset, concatenate_datasets
import wandb
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/volume1/YC/ai-models/logs/trainer/training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class TrainingConfig:
    """è®­ç»ƒé…ç½®ç±»"""
    model_name: str
    base_model: str
    dataset_path: str
    output_dir: str
    max_length: int = 512
    batch_size: int = 4
    learning_rate: float = 2e-5
    num_epochs: int = 3
    warmup_steps: int = 100
    save_steps: int = 500
    eval_steps: int = 500
    logging_steps: int = 100
    gradient_accumulation_steps: int = 1
    fp16: bool = True
    use_wandb: bool = False
    wandb_project: str = "yc-custom-models"
    use_lora: bool = False  # ä½¿ç”¨LoRAè¿›è¡Œé«˜æ•ˆå¾®è°ƒ
    quantize: bool = False  # é‡åŒ–æ¨¡åž‹ä»¥èŠ‚çœæ˜¾å­˜
    resume_from_checkpoint: Optional[str] = None  # ä»Žæ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ

class CustomModelTrainer:
    """è‡ªå®šä¹‰æ¨¡åž‹è®­ç»ƒå™¨"""
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.tokenizer = None
        self.model = None
        self.train_dataset = None
        self.eval_dataset = None
        self.training_stats = {
            "start_time": datetime.now().isoformat(),
            "end_time": None,
            "total_steps": 0,
            "training_loss": [],
            "eval_loss": [],
            "epoch_stats": []
        }
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        Path(config.output_dir).mkdir(parents=True, exist_ok=True)
        Path(os.path.join(config.output_dir, "checkpoints")).mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–wandb
        if config.use_wandb:
            wandb.init(
                project=config.wandb_project,
                name=config.model_name,
                config=config.__dict__
            )
    
    def load_model_and_tokenizer(self):
        """åŠ è½½åŸºç¡€æ¨¡åž‹å’Œåˆ†è¯å™¨"""
        logger.info(f"åŠ è½½åŸºç¡€æ¨¡åž‹: {self.config.base_model}")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.config.base_model,
                trust_remote_code=True
            )
            
            # è®¾ç½®pad_token
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
            
            # é‡åŒ–é…ç½®
            quantization_config = None
            if self.config.quantize:
                quantization_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_use_double_quant=True,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_compute_dtype=torch.float16
                )
            
            # åŠ è½½æ¨¡åž‹
            self.model = AutoModelForCausalLM.from_pretrained(
                self.config.base_model,
                trust_remote_code=True,
                torch_dtype=torch.float16 if self.config.fp16 else torch.float32,
                device_map="auto",
                quantization_config=quantization_config if self.config.quantize else None
            )
            
            # å¦‚æžœä½¿ç”¨LoRA
            if self.config.use_lora:
                logger.info("åº”ç”¨LoRAé…ç½®...")
                lora_config = LoraConfig(
                    r=16,
                    lora_alpha=32,
                    target_modules=["q_proj", "v_proj"],
                    lora_dropout=0.05,
                    bias="none",
                    task_type="CAUSAL_LM"
                )
                
                # å‡†å¤‡æ¨¡åž‹è¿›è¡Œé‡åŒ–è®­ç»ƒ
                if self.config.quantize:
                    self.model = prepare_model_for_kbit_training(self.model)
                    
                self.model = get_peft_model(self.model, lora_config)
                self.model.print_trainable_parameters()
            
            logger.info("æ¨¡åž‹å’Œåˆ†è¯å™¨åŠ è½½æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡åž‹å¤±è´¥: {e}")
            raise
    
    def prepare_dataset(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®é›†"""
        logger.info(f"å‡†å¤‡æ•°æ®é›†: {self.config.dataset_path}")
        
        try:
            # åŠ è½½æ•°æ®é›†
            datasets = []
            
            # æ”¯æŒå¤šä¸ªæ•°æ®é›†æ–‡ä»¶
            if os.path.isdir(self.config.dataset_path):
                # å¦‚æžœæ˜¯ç›®å½•ï¼ŒåŠ è½½æ‰€æœ‰JSONæ–‡ä»¶
                for filename in os.listdir(self.config.dataset_path):
                    if filename.endswith('.json'):
                        file_path = os.path.join(self.config.dataset_path, filename)
                        logger.info(f"åŠ è½½æ•°æ®é›†æ–‡ä»¶: {file_path}")
                        with open(file_path, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                        datasets.append(Dataset.from_list(data))
            else:
                # å¦‚æžœæ˜¯æ–‡ä»¶ï¼Œç›´æŽ¥åŠ è½½
                if self.config.dataset_path.endswith('.json'):
                    with open(self.config.dataset_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                    datasets.append(Dataset.from_list(data))
                else:
                    # å°è¯•ä»ŽHugging Face HubåŠ è½½
                    datasets.append(load_dataset(self.config.dataset_path)['train'])
            
            # åˆå¹¶æ•°æ®é›†
            dataset = concatenate_datasets(datasets)
            
            # æ•°æ®é¢„å¤„ç†
            def preprocess_function(examples):
                # æ”¯æŒå¤šç§æ•°æ®æ ¼å¼
                texts = []
                
                # æ ¼å¼1: {"input": "é—®é¢˜", "output": "ç­”æ¡ˆ"}
                if 'input' in examples and 'output' in examples:
                    for i in range(len(examples['input'])):
                        text = f"é—®é¢˜: {examples['input'][i]}\nç­”æ¡ˆ: {examples['output'][i]}"
                        texts.append(text)
                # æ ¼å¼2: {"prompt": "æç¤º", "response": "å›žåº”"}
                elif 'prompt' in examples and 'response' in examples:
                    for i in range(len(examples['prompt'])):
                        text = f"{examples['prompt'][i]}\n{examples['response'][i]}"
                        texts.append(text)
                # æ ¼å¼3: {"text": "å®Œæ•´æ–‡æœ¬"}
                elif 'text' in examples:
                    texts = examples['text']
                else:
                    raise ValueError("ä¸æ”¯æŒçš„æ•°æ®é›†æ ¼å¼")
                
                # åˆ†è¯
                tokenized = self.tokenizer(
                    texts,
                    truncation=True,
                    padding=True,
                    max_length=self.config.max_length,
                    return_tensors="pt"
                )
                
                # è®¾ç½®labels
                tokenized["labels"] = tokenized["input_ids"].clone()
                
                return tokenized
            
            # åº”ç”¨é¢„å¤„ç†
            tokenized_dataset = dataset.map(
                preprocess_function,
                batched=True,
                remove_columns=dataset.column_names
            )
            
            # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
            split_dataset = tokenized_dataset.train_test_split(test_size=0.1)
            self.train_dataset = split_dataset['train']
            self.eval_dataset = split_dataset['test']
            
            logger.info(f"æ•°æ®é›†å‡†å¤‡å®Œæˆ - è®­ç»ƒé›†: {len(self.train_dataset)}, éªŒè¯é›†: {len(self.eval_dataset)}")
            
        except Exception as e:
            logger.error(f"æ•°æ®é›†å‡†å¤‡å¤±è´¥: {e}")
            raise
    
    def train(self):
        """å¼€å§‹è®­ç»ƒ"""
        logger.info("å¼€å§‹æ¨¡åž‹è®­ç»ƒ")
        
        try:
            # è®­ç»ƒå‚æ•°
            training_args = TrainingArguments(
                output_dir=os.path.join(self.config.output_dir, "checkpoints"),
                overwrite_output_dir=True,
                num_train_epochs=self.config.num_epochs,
                per_device_train_batch_size=self.config.batch_size,
                per_device_eval_batch_size=self.config.batch_size,
                gradient_accumulation_steps=self.config.gradient_accumulation_steps,
                learning_rate=self.config.learning_rate,
                warmup_steps=self.config.warmup_steps,
                logging_steps=self.config.logging_steps,
                save_steps=self.config.save_steps,
                eval_steps=self.config.eval_steps,
                evaluation_strategy="steps",
                save_strategy="steps",
                load_best_model_at_end=True,
                metric_for_best_model="eval_loss",
                greater_is_better=False,
                fp16=self.config.fp16,
                dataloader_pin_memory=False,
                report_to="wandb" if self.config.use_wandb else None,
                run_name=self.config.model_name,
                resume_from_checkpoint=self.config.resume_from_checkpoint
            )
            
            # å®šä¹‰è®­ç»ƒå›žè°ƒä»¥æ”¶é›†ç»Ÿè®¡ä¿¡æ¯
            def log_trainer_callback(args, state, control, **kwargs):
                self.training_stats["total_steps"] = state.global_step
                if state.is_local_process_zero:
                    if state.training_loss is not None:
                        self.training_stats["training_loss"].append({
                            "step": state.global_step,
                            "loss": state.training_loss
                        })
            
            # æ•°æ®æ•´ç†å™¨
            data_collator = DataCollatorForLanguageModeling(
                tokenizer=self.tokenizer,
                mlm=False
            )
            
            # åˆ›å»ºè®­ç»ƒå™¨
            trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=self.train_dataset,
                eval_dataset=self.eval_dataset,
                data_collator=data_collator,
                tokenizer=self.tokenizer,
                callbacks=[log_trainer_callback]
            )
            
            # å¼€å§‹è®­ç»ƒ
            trainer.train(resume_from_checkpoint=self.config.resume_from_checkpoint)
            
            # ä¿å­˜æœ€ç»ˆæ¨¡åž‹
            logger.info("ä¿å­˜æœ€ç»ˆæ¨¡åž‹...")
            if self.config.use_lora:
                # å¯¹äºŽLoRAæ¨¡åž‹ï¼Œä¿å­˜Peftæ¨¡åž‹
                self.model.save_pretrained(os.path.join(self.config.output_dir, "lora_weights"))
                # åŒæ—¶ä¿å­˜åŸºç¡€æ¨¡åž‹ä¿¡æ¯
                with open(os.path.join(self.config.output_dir, "base_model_info.txt"), "w") as f:
                    f.write(self.config.base_model)
            else:
                # ä¿å­˜å®Œæ•´æ¨¡åž‹
                trainer.save_model(self.config.output_dir)
            
            self.tokenizer.save_pretrained(self.config.output_dir)
            
            # ä¿å­˜è®­ç»ƒé…ç½®
            config_path = os.path.join(self.config.output_dir, "training_config.json")
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(self.config.__dict__, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜è®­ç»ƒç»Ÿè®¡
            self.training_stats["end_time"] = datetime.now().isoformat()
            stats_path = os.path.join(self.config.output_dir, "training_stats.json")
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(self.training_stats, f, indent=2, ensure_ascii=False)
            
            logger.info(f"è®­ç»ƒå®Œæˆï¼Œæ¨¡åž‹ä¿å­˜è‡³: {self.config.output_dir}")
            
        except Exception as e:
            logger.error(f"è®­ç»ƒå¤±è´¥: {e}")
            raise
    
    def evaluate(self):
        """è¯„ä¼°æ¨¡åž‹"""
        logger.info("å¼€å§‹æ¨¡åž‹è¯„ä¼°")
        
        try:
            # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡åž‹
            if self.config.use_lora:
                from peft import PeftModel
                
                # åŠ è½½åŸºç¡€æ¨¡åž‹
                base_model = AutoModelForCausalLM.from_pretrained(
                    self.config.base_model,
                    trust_remote_code=True,
                    torch_dtype=torch.float16 if self.config.fp16 else torch.float32,
                    device_map="auto"
                )
                
                # åŠ è½½LoRAæƒé‡
                self.model = PeftModel.from_pretrained(
                    base_model,
                    os.path.join(self.config.output_dir, "lora_weights")
                )
                self.model.eval()
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.config.output_dir,
                    trust_remote_code=True,
                    device_map="auto"
                )
            
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.output_dir)
            
            # ç®€å•çš„ç”Ÿæˆæµ‹è¯•
            test_inputs = [
                "é—®é¢˜: ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
                "é—®é¢˜: å¦‚ä½•å­¦ä¹ æœºå™¨å­¦ä¹ ï¼Ÿ",
                "é—®é¢˜: Pythonæœ‰ä»€ä¹ˆä¼˜åŠ¿ï¼Ÿ"
            ]
            
            results = []
            for test_input in test_inputs:
                inputs = self.tokenizer(test_input, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_length=200,
                        num_return_sequences=1,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                
                generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                results.append({
                    "input": test_input,
                    "output": generated_text
                })
            
            # ä¿å­˜è¯„ä¼°ç»“æžœ
            eval_path = os.path.join(self.config.output_dir, "evaluation_results.json")
            with open(eval_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            
            logger.info(f"è¯„ä¼°å®Œæˆï¼Œç»“æžœä¿å­˜è‡³: {eval_path}")
            return results
            
        except Exception as e:
            logger.error(f"è¯„ä¼°å¤±è´¥: {e}")
            raise

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="è‡ªå®šä¹‰æ¨¡åž‹è®­ç»ƒ")
    parser.add_argument("--config", type=str, required=True, help="è®­ç»ƒé…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--only-evaluate", action="store_true", help="ä»…è¿›è¡Œè¯„ä¼°ï¼Œä¸è®­ç»ƒ")
    args = parser.parse_args()
    
    # åŠ è½½é…ç½®
    with open(args.config, 'r', encoding='utf-8') as f:
        config_dict = json.load(f)
    
    config = TrainingConfig(** config_dict)
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = CustomModelTrainer(config)
    
    # æ‰§è¡Œè®­ç»ƒæµç¨‹
    trainer.load_model_and_tokenizer()
    
    if not args.only_evaluate:
        trainer.prepare_dataset()
        trainer.train()
    
    trainer.evaluate()
    
    logger.info("è®­ç»ƒæµç¨‹å®Œæˆ")

if __name__ == "__main__":
    main()
EOF

    # åˆ›å»ºè®­ç»ƒé…ç½®æ¨¡æ¿
    cat > "$training_dir/config_template.json" << 'EOF'
{
    "model_name": "yc-custom-model-v1",
    "base_model": "Qwen/Qwen2-1.5B-Instruct",
    "dataset_path": "/volume1/YC/ai-models/training/datasets/custom_dataset.json",
    "output_dir": "/volume1/YC/ai-models/training/models/yc-custom-model-v1",
    "max_length": 512,
    "batch_size": 4,
    "learning_rate": 2e-5,
    "num_epochs": 3,
    "warmup_steps": 100,
    "save_steps": 500,
    "eval_steps": 500,
    "logging_steps": 100,
    "gradient_accumulation_steps": 1,
    "fp16": true,
    "use_wandb": false,
    "wandb_project": "yc-custom-models",
    "use_lora": true,
    "quantize": true,
    "resume_from_checkpoint": null
}
EOF

    # åˆ›å»ºæ•°æ®é›†ç¤ºä¾‹
    cat > "$training_dir/datasets/sample_dataset.json" << 'EOF'
[
    {
        "input": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
        "output": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜Žç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚é€šè¿‡ç®—æ³•å’Œç»Ÿè®¡æ¨¡åž‹ï¼Œæœºå™¨å­¦ä¹ ç³»ç»Ÿå¯ä»¥ä»Žæ•°æ®ä¸­è¯†åˆ«æ¨¡å¼å¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚"
    },
    {
        "input": "æ·±åº¦å­¦ä¹ å’Œæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "output": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥žç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å·¥ä½œæ–¹å¼ã€‚ä¸Žä¼ ç»Ÿæœºå™¨å­¦ä¹ ç›¸æ¯”ï¼Œæ·±åº¦å­¦ä¹ èƒ½å¤Ÿè‡ªåŠ¨æå–ç‰¹å¾ï¼Œå¤„ç†æ›´å¤æ‚çš„æ•°æ®ï¼Œå¦‚å›¾åƒã€è¯­éŸ³å’Œè‡ªç„¶è¯­è¨€ã€‚"
    },
    {
        "input": "å¦‚ä½•å¼€å§‹å­¦ä¹ äººå·¥æ™ºèƒ½ï¼Ÿ",
        "output": "å­¦ä¹ äººå·¥æ™ºèƒ½å»ºè®®ä»Žä»¥ä¸‹æ­¥éª¤å¼€å§‹ï¼š1. æŽŒæ¡æ•°å­¦åŸºç¡€ï¼ˆçº¿æ€§ä»£æ•°ã€æ¦‚çŽ‡ç»Ÿè®¡ã€å¾®ç§¯åˆ†ï¼‰ï¼›2. å­¦ä¹ ç¼–ç¨‹è¯­è¨€ï¼ˆPythonæŽ¨èï¼‰ï¼›3. äº†è§£æœºå™¨å­¦ä¹ åŸºç¡€ç†è®ºï¼›4. å®žè·µé¡¹ç›®å’Œæ¡ˆä¾‹ï¼›5. æ·±å…¥ç‰¹å®šé¢†åŸŸï¼ˆå¦‚è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰ï¼‰ã€‚"
    },
    {
        "input": "ä»€ä¹ˆæ˜¯ç¥žç»ç½‘ç»œï¼Ÿ",
        "output": "ç¥žç»ç½‘ç»œæ˜¯å—äººè„‘ç»“æž„å¯å‘çš„è®¡ç®—æ¨¡åž‹ï¼Œç”±ç›¸äº’è¿žæŽ¥çš„èŠ‚ç‚¹ï¼ˆç¥žç»å…ƒï¼‰ç»„æˆã€‚è¿™äº›ç½‘ç»œé€šè¿‡è°ƒæ•´èŠ‚ç‚¹ä¹‹é—´çš„è¿žæŽ¥å¼ºåº¦ï¼ˆæƒé‡ï¼‰æ¥å­¦ä¹ ä»Žè¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„ï¼Œæ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒç»„æˆéƒ¨åˆ†ã€‚"
    },
    {
        "input": "ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
        "output": "ç›‘ç£å­¦ä¹ ä½¿ç”¨æ ‡è®°æ•°æ®ï¼ˆå¸¦æœ‰å·²çŸ¥è¾“å‡ºçš„è¾“å…¥ï¼‰è¿›è¡Œè®­ç»ƒï¼Œç›®æ ‡æ˜¯å­¦ä¹ è¾“å…¥åˆ°è¾“å‡ºçš„æ˜ å°„ã€‚æ— ç›‘ç£å­¦ä¹ åˆ™ä½¿ç”¨æœªæ ‡è®°æ•°æ®ï¼Œç›®æ ‡æ˜¯å‘çŽ°æ•°æ®ä¸­éšè—çš„æ¨¡å¼æˆ–ç»“æž„ï¼Œå¦‚èšç±»åˆ†æžã€‚"
    }
]
EOF

    # åˆ›å»ºè®­ç»ƒå¯åŠ¨è„šæœ¬
    cat > "$training_dir/start_training.sh" << 'EOF'
#!/bin/bash

# è®­ç»ƒå¯åŠ¨è„šæœ¬
set -euo pipefail

# æ—¥å¿—é…ç½®
LOG_DIR="/volume1/YC/ai-models/logs/trainer"
mkdir -p "$LOG_DIR"
LOG_FILE="$LOG_DIR/training_$(date +%Y%m%d_%H%M%S).log"

# æ˜¾ç¤ºå¸®åŠ©
show_help() {
    echo "ç”¨æ³•: $0 [é€‰é¡¹] <é…ç½®æ–‡ä»¶è·¯å¾„>"
    echo "é€‰é¡¹:"
    echo "  -h, --help      æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"
    echo "  -e, --evaluate  ä»…è¿›è¡Œè¯„ä¼°ï¼Œä¸è®­ç»ƒ"
    echo "  -d, --debug     æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯"
    echo ""
}

# è§£æžå‚æ•°
EVAL_ONLY=false
DEBUG=false
CONFIG_FILE=""

while [[ $# -gt 0 ]]; do
    case "$1" in
        -h|--help)
            show_help
            exit 0
            ;;
        -e|--evaluate)
            EVAL_ONLY=true
            shift
            ;;
        -d|--debug)
            DEBUG=true
            shift
            ;;
        *)
            if [ -z "$CONFIG_FILE" ]; then
                CONFIG_FILE="$1"
                shift
            else
                echo "é”™è¯¯: æœªçŸ¥å‚æ•° $1"
                show_help
                exit 1
            fi
            ;;
    esac
done

# æ£€æŸ¥é…ç½®æ–‡ä»¶
if [ -z "$CONFIG_FILE" ] || [ ! -f "$CONFIG_FILE" ]; then
    echo "é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $CONFIG_FILE"
    show_help
    exit 1
fi

echo "å¼€å§‹è®­ç»ƒæµç¨‹ï¼Œæ—¥å¿—æ–‡ä»¶: $LOG_FILE"

# æž„å»ºå‘½ä»¤
CMD="python3 /volume1/YC/ai-models/training/trainer.py --config \"$CONFIG_FILE\""
if [ "$EVAL_ONLY" = true ]; then
    CMD="$CMD --only-evaluate"
fi

# æ‰§è¡Œå‘½ä»¤
if [ "$DEBUG" = true ]; then
    echo "æ‰§è¡Œå‘½ä»¤: $CMD"
    $CMD | tee "$LOG_FILE"
else
    $CMD > "$LOG_FILE" 2>&1
    echo "è®­ç»ƒæµç¨‹å·²å¯åŠ¨ï¼ŒæŸ¥çœ‹æ—¥å¿—: tail -f $LOG_FILE"
    echo "è®­ç»ƒçŠ¶æ€: è¿è¡Œä¸­"
fi

# æ£€æŸ¥æ‰§è¡Œç»“æžœ
if [ $? -eq 0 ]; then
    echo "è®­ç»ƒæµç¨‹æˆåŠŸå®Œæˆ"
    exit 0
else
    echo "è®­ç»ƒæµç¨‹å¤±è´¥ï¼ŒæŸ¥çœ‹æ—¥å¿—èŽ·å–è¯¦æƒ…: $LOG_FILE"
    exit 1
fi
EOF

chmod +x "$training_dir/start_training.sh"

    log_success "è‡ªå®šä¹‰æ¨¡åž‹è®­ç»ƒç³»ç»Ÿåˆ›å»ºå®Œæˆ"
}

# åˆ›å»ºç§»åŠ¨ç«¯åº”ç”¨
create_mobile_app() {
    log_step "åˆ›å»ºç§»åŠ¨ç«¯åº”ç”¨..."
    
    local mobile_dir="$AI_DIR/../mobile-app"
    mkdir -p "$mobile_dir"/{
        src/{components,screens,services,utils,store},
        assets/{images,fonts},
        android,
        ios,
        __tests__
    }
    
    # åˆ›å»ºç§»åŠ¨ç«¯ä¸»åº”ç”¨
    cat > "$mobile_dir/App.js" << 'EOF'
import React from 'react';
import { NavigationContainer } from '@react-navigation/native';
import { createBottomTabNavigator } from '@react-navigation/bottom-tabs';
import { createStackNavigator } from '@react-navigation/stack';
import { Provider } from 'react-redux';
import { PersistGate } from 'redux-persist/integration/react';
import Icon from 'react-native-vector-icons/MaterialIcons';
import { LogBox, View } from 'react-native';

// å¿½ç•¥è­¦å‘Š
LogBox.ignoreLogs([
  'Non-serializable values were found in the navigation state',
  'VirtualizedLists should never be nested'
]);

import { store, persistor } from './src/store';
import HomeScreen from './src/screens/HomeScreen';
import ChatScreen from './src/screens/ChatScreen';
import ModelsScreen from './src/screens/ModelsScreen';
import FilesScreen from './src/screens/FilesScreen';
import SettingsScreen from './src/screens/SettingsScreen';
import LoginScreen from './src/screens/LoginScreen';
import TrainingScreen from './src/screens/TrainingScreen';
import LoadingScreen from './src/components/LoadingScreen';
import SplashScreen from './src/components/SplashScreen';

const Tab = createBottomTabNavigator();
const Stack = createStackNavigator();

// ä¸»æ ‡ç­¾å¯¼èˆª
function MainTabs() {
  return (
    <Tab.Navigator
      screenOptions={({ route }) => ({
        tabBarIcon: ({ focused, color, size }) => {
          let iconName;

          if (route.name === 'é¦–é¡µ') {
            iconName = 'home';
          } else if (route.name === 'AIèŠå¤©') {
            iconName = 'chat';
          } else if (route.name === 'æ¨¡åž‹ç®¡ç†') {
            iconName = 'settings_suggest';
          } else if (route.name === 'æ–‡ä»¶') {
            iconName = 'folder';
          } else if (route.name === 'è®­ç»ƒ') {
            iconName = 'auto_awesome';
          }

          return <Icon name={iconName} size={size} color={color} />;
        },
        tabBarActiveTintColor: '#007AFF',
        tabBarInactiveTintColor: 'gray',
        headerStyle: {
          backgroundColor: '#007AFF',
        },
        headerTintColor: '#fff',
        headerTitleStyle: {
          fontWeight: 'bold',
        },
      })}
    >
      <Tab.Screen name="é¦–é¡µ" component={HomeScreen} />
      <Tab.Screen name="AIèŠå¤©" component={ChatScreen} />
      <Tab.Screen name="æ¨¡åž‹ç®¡ç†" component={ModelsScreen} />
      <Tab.Screen name="æ–‡ä»¶" component={FilesScreen} />
      <Tab.Screen name="è®­ç»ƒ" component={TrainingScreen} />
    </Tab.Navigator>
  );
}

// ä¸»åº”ç”¨å¯¼èˆª
function AppNavigator() {
  return (
    <Stack.Navigator initialRouteName="Splash">
      <Stack.Screen 
        name="Splash" 
        component={SplashScreen} 
        options={{ headerShown: false }}
      />
      <Stack.Screen 
        name="Login" 
        component={LoginScreen} 
        options={{ headerShown: false }}
      />
      <Stack.Screen 
        name="Main" 
        component={MainTabs} 
        options={{ headerShown: false }}
      />
      <Stack.Screen 
        name="Settings" 
        component={SettingsScreen} 
        options={{ 
          title: 'è®¾ç½®',
          headerBackTitleVisible: false
        }}
      />
    </Stack.Navigator>
  );
}

export default function App() {
  return (
    <Provider store={store}>
      <PersistGate loading={<LoadingScreen />} persistor={persistor}>
        <NavigationContainer>
          <AppNavigator />
        </NavigationContainer>
      </PersistGate>
    </Provider>
  );
}
EOF

    # åˆ›å»ºAIèŠå¤©ç•Œé¢
    cat > "$mobile_dir/src/screens/ChatScreen.js" << 'EOF'
import React, { useState, useEffect, useRef } from 'react';
import {
  View,
  Text,
  TextInput,
  TouchableOpacity,
  FlatList,
  StyleSheet,
  Alert,
  KeyboardAvoidingView,
  Platform,
  ActivityIndicator,
  ScrollView,
  SafeAreaView
} from 'react-native';
import Icon from 'react-native-vector-icons/MaterialIcons';
import { useSelector, useDispatch } from 'react-redux';
import { useNavigation } from '@react-navigation/native';

import { sendMessage, clearMessages, setCurrentModel } from '../store/chatSlice';
import { AIService } from '../services/AIService';
import MessageItem from '../components/MessageItem';
import ModelSelector from '../components/ModelSelector';
import AttachmentButton from '../components/AttachmentButton';

export default function ChatScreen() {
  const [inputText, setInputText] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [showModelSelector, setShowModelSelector] = useState(false);
  const flatListRef = useRef(null);
  
  const dispatch = useDispatch();
  const navigation = useNavigation();
  const { messages, models, currentModel } = useSelector(state => state.chat);
  const { user } = useSelector(state => state.auth);
  const { serverUrl } = useSelector(state => state.settings);

  useEffect(() => {
    // æ»šåŠ¨åˆ°åº•éƒ¨
    if (messages.length > 0) {
      flatListRef.current?.scrollToEnd({ animated: true });
    }
  }, [messages]);

  // åŠ è½½æ¨¡åž‹åˆ—è¡¨
  useEffect(() => {
    const loadModels = async () => {
      try {
        const response = await AIService.getModels();
        // å¯ä»¥åœ¨è¿™é‡Œå¤„ç†æ¨¡åž‹åˆ—è¡¨
      } catch (error) {
        console.error('åŠ è½½æ¨¡åž‹åˆ—è¡¨å¤±è´¥:', error);
        Alert.alert('é”™è¯¯', 'åŠ è½½æ¨¡åž‹åˆ—è¡¨å¤±è´¥ï¼Œè¯·æ£€æŸ¥è¿žæŽ¥');
      }
    };

    loadModels();
  }, [serverUrl]);

  const handleSendMessage = async () => {
    if (!inputText.trim() || isLoading) return;

    const userMessage = {
      id: Date.now().toString(),
      text: inputText.trim(),
      sender: 'user',
      timestamp: new Date().toISOString</doubaocanvas>